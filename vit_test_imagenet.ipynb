{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56051b0c9576563e",
   "metadata": {},
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.proj = nn.Linear(in_chans * patch_size ** 2, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H % self.patch_size == 0 and W % self.patch_size == 0\n",
    "        x = x.reshape(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 4, 1, 3, 5)\n",
    "        x = x.reshape(B, -1, C * self.patch_size * self.patch_size)\n",
    "        return self.proj(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0251bca53f2b88f",
   "metadata": {},
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=True)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        H = self.num_heads\n",
    "        Hd = self.head_dim\n",
    "        q = q.view(B, N, H, Hd).transpose(1, 2)  # (B, H, N, Hd)\n",
    "        k = k.view(B, N, H, Hd).transpose(1, 2)\n",
    "        v = v.view(B, N, H, Hd).transpose(1, 2)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = attn @ v  # (B, H, N, Hd)\n",
    "        out = out.transpose(1, 2).reshape(B, N, D)  # (B, N, D)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6a6b5e7103de823",
   "metadata": {},
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim=768, mlp_ratio=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim * mlp_ratio)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(embed_dim * mlp_ratio, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8724da7ef96211f",
   "metadata": {},
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim=embed_dim, mlp_ratio=mlp_ratio, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x)) + x\n",
    "        x = self.mlp(self.norm2(x)) + x\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b99da044885ec362",
   "metadata": {},
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size=224, n_classes=1000, depth=12, embed_dim=768, patch_size=16, num_heads=12, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(image_size=image_size, patch_size=patch_size, embed_dim=embed_dim)\n",
    "        self.num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim) * 0.02)\n",
    "        self.pos_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, dropout=dropout) for _ in range(depth)])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        cls_output = x[:, 0]\n",
    "        logits = self.head(cls_output)\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c0dc3ce3ba3ce7b",
   "metadata": {},
   "source": [
    "model = ViT()\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32f5ebc7bfa25f70",
   "metadata": {},
   "source": [
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf5d5bfa72d2a8a2",
   "metadata": {},
   "source": [
    "dummy_input = torch.randn(2, 3, 224, 224).to(device)\n",
    "output = model(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31f2201d7d80629a",
   "metadata": {},
   "source": [
    "# ImageNet normalization stats\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64943631da77a194",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Stream ImageNet from Hugging Face (requires authentication for gated dataset)\n",
    "# Run: huggingface-cli login\n",
    "train_dataset = load_dataset(\"ILSVRC/imagenet-1k\", split=\"train\", streaming=True)\n",
    "val_dataset = load_dataset(\"ILSVRC/imagenet-1k\", split=\"validation\", streaming=True)\n",
    "\n",
    "def transform_example(example, transform):\n",
    "    image = example[\"image\"].convert(\"RGB\")\n",
    "    return {\"image\": transform(image), \"label\": example[\"label\"]}\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x: transform_example(x, transform_train))\n",
    "val_dataset = val_dataset.map(lambda x: transform_example(x, transform_test))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([x[\"image\"] for x in batch])\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch])\n",
    "    return images, labels\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=4)\n",
    "test_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "print(\"Streaming ImageNet from Hugging Face\")\n",
    "print(f\"Number of classes: 1000\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "159b9f3ccf5eaa16",
   "metadata": {},
   "source": "def train_epoch(model, train_loader, optimizer, device, steps_per_epoch=20018):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    step = 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = nn.CrossEntropyLoss()(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n        step += 1\n        \n        if step >= steps_per_epoch:\n            break\n\n    return total_loss / step, 100 * correct / total",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c41ab5f642a73eed",
   "metadata": {},
   "source": "def evaluate(model, test_loader, device, steps=782):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    step = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n\n            test_loss += nn.CrossEntropyLoss()(outputs, labels).item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            step += 1\n            \n            if step >= steps:\n                break\n    return test_loss / step, 100 * correct / total",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd56dedb438b87",
   "metadata": {},
   "source": "import time\n\nn_epochs = 90\ntrain_losses, test_losses = [], []\ntrain_accs, test_accs = [], []\n\n# ImageNet: 1,281,167 train / 50,000 val images\n# With batch_size=64: ~20018 train steps, ~782 val steps per epoch\nTRAIN_STEPS = 20018\nVAL_STEPS = 782\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n\nfor epoch in range(n_epochs):\n    start_time = time.time()\n    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, steps_per_epoch=TRAIN_STEPS)\n    test_loss, test_acc = evaluate(model, test_loader, device, steps=VAL_STEPS)\n    elapsed_time = time.time() - start_time\n    scheduler.step()\n\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n    print(f\"Epoch {epoch+1:2d}/{n_epochs} | \"\n          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n          f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}% | \"\n          f\"Elapsed Time: {elapsed_time:.2f}s\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "822b48d7d1a5a7c4",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(train_losses, label=\"Training Loss\")\n",
    "ax1.plot(test_losses, label=\"Validation Loss\")\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_accs, label='Train')\n",
    "ax2.plot(test_accs, label='Test')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
