{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) from Scratch\n",
    "\n",
    "This notebook implements a Vision Transformer based on the paper:\n",
    "[\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "## Architecture Overview\n",
    "1. **Patch Embedding**: Split image into fixed-size patches and linearly embed them\n",
    "2. **Position Embedding**: Add learnable position embeddings\n",
    "3. **Transformer Encoder**: Stack of multi-head self-attention and MLP blocks\n",
    "4. **Classification Head**: MLP head on the [CLS] token for classification"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:22:57.997324Z",
     "start_time": "2025-12-24T05:22:57.994828Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Patch Embedding\n",
    "\n",
    "Convert an image into a sequence of flattened patches, then project to embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:22:58.017243Z",
     "start_time": "2025-12-24T05:22:58.014945Z"
    }
   },
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Split image into patches and embed them.\n",
    "    \n",
    "    Args:\n",
    "        img_size: Size of input image (assumed square)\n",
    "        patch_size: Size of each patch (assumed square)\n",
    "        in_channels: Number of input channels (3 for RGB)\n",
    "        embed_dim: Embedding dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Conv2d is equivalent to splitting into patches and linear projection\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.proj(x)  # (B, embed_dim, n_patches_h, n_patches_w)\n",
    "        x = x.flatten(2)  # (B, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Head Self-Attention\n",
    "\n",
    "The core attention mechanism that allows patches to attend to each other."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:22:58.024949Z",
     "start_time": "2025-12-24T05:22:58.022447Z"
    }
   },
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Embedding dimension\n",
    "        n_heads: Number of attention heads\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=768, n_heads=12, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        \n",
    "        assert embed_dim % n_heads == 0, \"embed_dim must be divisible by n_heads\"\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, n_heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, n_heads, N, N)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLP (Feed-Forward Network)\n",
    "\n",
    "Two-layer MLP with GELU activation, applied after attention in each transformer block."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:22:58.028902Z",
     "start_time": "2025-12-24T05:22:58.027013Z"
    }
   },
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed-forward network with GELU activation.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Input/output dimension\n",
    "        mlp_ratio: Ratio to determine hidden dimension\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=768, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer Encoder Block\n",
    "\n",
    "A single transformer block: LayerNorm -> Attention -> Residual -> LayerNorm -> MLP -> Residual"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:22:58.033516Z",
     "start_time": "2025-12-24T05:22:58.031645Z"
    }
   },
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block with pre-norm architecture.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Embedding dimension\n",
    "        n_heads: Number of attention heads\n",
    "        mlp_ratio: MLP hidden dimension ratio\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=768, n_heads=12, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, n_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pre-norm architecture (as in original ViT)\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Vision Transformer\n",
    "\n",
    "Putting it all together: Patch embedding + [CLS] token + Position embedding + Transformer blocks + Classification head"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:22:58.038544Z",
     "start_time": "2025-12-24T05:22:58.035314Z"
    }
   },
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer for image classification.\n",
    "    \n",
    "    Args:\n",
    "        img_size: Input image size (assumed square)\n",
    "        patch_size: Patch size (assumed square)\n",
    "        in_channels: Number of input channels\n",
    "        n_classes: Number of classification classes\n",
    "        embed_dim: Embedding dimension\n",
    "        depth: Number of transformer blocks\n",
    "        n_heads: Number of attention heads\n",
    "        mlp_ratio: MLP hidden dimension ratio\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        n_classes=1000,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        n_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        n_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        # Learnable [CLS] token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Learnable position embeddings (for patches + CLS token)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n",
    "        self.pos_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer encoder blocks\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, n_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Classification head\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        # Initialize position embeddings and CLS token\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "        # Initialize linear layers and layer norms\n",
    "        self.apply(self._init_module_weights)\n",
    "    \n",
    "    def _init_module_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (B, n_patches, embed_dim)\n",
    "        \n",
    "        # Prepend [CLS] token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, n_patches + 1, embed_dim)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        # Final norm and classification\n",
    "        x = self.norm(x)\n",
    "        cls_output = x[:, 0]  # Take [CLS] token output\n",
    "        logits = self.head(cls_output)\n",
    "        \n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Variants\n",
    "\n",
    "Helper functions to create standard ViT variants (ViT-Tiny, ViT-Small, ViT-Base, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:22:58.042516Z",
     "start_time": "2025-12-24T05:22:58.040549Z"
    }
   },
   "source": [
    "def vit_tiny(img_size=224, patch_size=16, n_classes=1000, **kwargs):\n",
    "    \"\"\"ViT-Tiny: 5.7M parameters\"\"\"\n",
    "    return VisionTransformer(\n",
    "        img_size=img_size, patch_size=patch_size, n_classes=n_classes,\n",
    "        embed_dim=192, depth=12, n_heads=3, **kwargs\n",
    "    )\n",
    "\n",
    "def vit_small(img_size=224, patch_size=16, n_classes=1000, **kwargs):\n",
    "    \"\"\"ViT-Small: 22M parameters\"\"\"\n",
    "    return VisionTransformer(\n",
    "        img_size=img_size, patch_size=patch_size, n_classes=n_classes,\n",
    "        embed_dim=384, depth=12, n_heads=6, **kwargs\n",
    "    )\n",
    "\n",
    "def vit_base(img_size=224, patch_size=16, n_classes=1000, **kwargs):\n",
    "    \"\"\"ViT-Base: 86M parameters\"\"\"\n",
    "    return VisionTransformer(\n",
    "        img_size=img_size, patch_size=patch_size, n_classes=n_classes,\n",
    "        embed_dim=768, depth=12, n_heads=12, **kwargs\n",
    "    )\n",
    "\n",
    "def vit_large(img_size=224, patch_size=16, n_classes=1000, **kwargs):\n",
    "    \"\"\"ViT-Large: 307M parameters\"\"\"\n",
    "    return VisionTransformer(\n",
    "        img_size=img_size, patch_size=patch_size, n_classes=n_classes,\n",
    "        embed_dim=1024, depth=24, n_heads=16, **kwargs\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Model\n",
    "\n",
    "Let's verify our implementation works with a dummy input."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T05:22:59.481110Z",
     "start_time": "2025-12-24T05:22:58.810805Z"
    }
   },
   "source": [
    "# Create a small ViT for testing\n",
    "model = vit_tiny(img_size=32, patch_size=4, n_classes=10)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(2, 3, 32, 32).to(device)\n",
    "output = model(dummy_input)\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 5,362,762\n",
      "Input shape: torch.Size([2, 3, 32, 32])\n",
      "Output shape: torch.Size([2, 10])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train on CIFAR-10\n",
    "\n",
    "Let's train our ViT on CIFAR-10 to verify it learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(test_loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model for CIFAR-10 (32x32 images, 10 classes)\n",
    "model = vit_tiny(img_size=32, patch_size=4, n_classes=10, dropout=0.1)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "print(f\"Training ViT-Tiny on CIFAR-10\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 20\n",
    "train_losses, test_losses = [], []\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{n_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(train_losses, label='Train')\n",
    "ax1.plot(test_losses, label='Test')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_accs, label='Train')\n",
    "ax2.plot(test_accs, label='Test')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Attention Maps\n",
    "\n",
    "Let's visualize what the model is attending to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_maps(model, x):\n",
    "    \"\"\"Extract attention maps from the model.\"\"\"\n",
    "    attention_maps = []\n",
    "    \n",
    "    # Register hooks to capture attention weights\n",
    "    def hook_fn(module, input, output):\n",
    "        # Get attention weights before softmax\n",
    "        B, N, C = input[0].shape\n",
    "        qkv = module.qkv(input[0]).reshape(B, N, 3, module.n_heads, module.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * module.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attention_maps.append(attn.detach().cpu())\n",
    "    \n",
    "    hooks = []\n",
    "    for block in model.blocks:\n",
    "        hook = block.attn.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample image\n",
    "sample_img, sample_label = test_dataset[0]\n",
    "sample_img_batch = sample_img.unsqueeze(0).to(device)\n",
    "\n",
    "# Get attention maps\n",
    "attn_maps = get_attention_maps(model, sample_img_batch)\n",
    "\n",
    "# Visualize attention from CLS token to patches in the last layer\n",
    "last_attn = attn_maps[-1][0]  # (n_heads, N, N)\n",
    "cls_attn = last_attn[:, 0, 1:]  # Attention from CLS to patches\n",
    "\n",
    "# Average across heads\n",
    "cls_attn_avg = cls_attn.mean(0)  # (n_patches,)\n",
    "n_patches_side = int(cls_attn_avg.shape[0] ** 0.5)\n",
    "cls_attn_map = cls_attn_avg.reshape(n_patches_side, n_patches_side)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Original image\n",
    "img_np = sample_img.permute(1, 2, 0).numpy()\n",
    "img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "axes[0].imshow(img_np)\n",
    "axes[0].set_title(f'Original (Label: {sample_label})')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Attention map\n",
    "axes[1].imshow(cls_attn_map.numpy(), cmap='viridis')\n",
    "axes[1].set_title('CLS Attention (Last Layer)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Overlay\n",
    "attn_resized = np.array(cls_attn_map)\n",
    "attn_resized = np.kron(attn_resized, np.ones((4, 4)))  # Upsample to match image size\n",
    "axes[2].imshow(img_np)\n",
    "axes[2].imshow(attn_resized, cmap='jet', alpha=0.5)\n",
    "axes[2].set_title('Attention Overlay')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Experiment Ideas\n",
    "\n",
    "Now that you have a working ViT, here are some things to try:\n",
    "\n",
    "1. **Different patch sizes**: Try `patch_size=2` or `patch_size=8` and see how it affects performance\n",
    "2. **Model scaling**: Compare `vit_tiny`, `vit_small`, and `vit_base` on CIFAR-10\n",
    "3. **Data augmentation**: Add more augmentation like `RandAugment` or `Mixup`\n",
    "4. **Learning rate warmup**: Add warmup for more stable training\n",
    "5. **Different datasets**: Try CIFAR-100 or download a subset of ImageNet\n",
    "6. **Positional encoding variations**: Try sinusoidal instead of learned position embeddings\n",
    "7. **Attention visualization**: Compare attention patterns at different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare different patch sizes\n",
    "for patch_size in [2, 4, 8]:\n",
    "    test_model = vit_tiny(img_size=32, patch_size=patch_size, n_classes=10)\n",
    "    n_params = sum(p.numel() for p in test_model.parameters())\n",
    "    n_patches = (32 // patch_size) ** 2\n",
    "    print(f\"Patch size: {patch_size} | Patches: {n_patches} | Parameters: {n_params:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
